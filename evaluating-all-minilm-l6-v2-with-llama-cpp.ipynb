{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nThis note evaluate all-MiniLM-L6-v2 with llama.cpp\n\n# Use pre-complie llama.cpp","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/evaluating-smollm-135m-instruct-gguf-with-llamacpp/llama.cpp /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:33:15.446169Z","iopub.execute_input":"2024-09-26T09:33:15.446547Z","iopub.status.idle":"2024-09-26T09:33:59.239179Z","shell.execute_reply.started":"2024-09-26T09:33:15.446509Z","shell.execute_reply":"2024-09-26T09:33:59.237996Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!chmod +x -R llama.cpp\n%cd llama.cpp\n!pip install --force-reinstall -r requirements.txt > pip_install.log 2>&1\n!tail pip_install.log","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:33:59.241212Z","iopub.execute_input":"2024-09-26T09:33:59.241574Z","iopub.status.idle":"2024-09-26T09:36:05.734176Z","shell.execute_reply.started":"2024-09-26T09:33:59.241540Z","shell.execute_reply":"2024-09-26T09:36:05.733046Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/llama.cpp\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.0 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ns3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2024.9.0 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-2.1.5 certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.1 fsspec-2024.9.0 gguf-0.10.0 huggingface-hub-0.25.1 idna-3.10 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 packaging-24.1 protobuf-4.25.3 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 sentencepiece-0.2.0 sympy-1.13.3 tokenizers-0.20.0 torch-2.2.2+cpu tqdm-4.66.5 transformers-4.45.0 typing-extensions-4.12.2 urllib3-2.2.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Run perplexity\nPerplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens. Formally, the perplexity of LLMs is the exponentiated average negative log-likelihood, which means perplexity smaller is better\n\nPerplexity benchmark shows how accurate the model is","metadata":{}},{"cell_type":"code","source":"!/kaggle/working/llama.cpp/llama-perplexity -m /kaggle/input/all-minilm-l6-v2/gguf/fp16/1/all-MiniLM-L6-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_L6_fp16.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m /kaggle/input/all-minilm-l6-v2/gguf/q4_k_m/1/all-MiniLM-L6-v2-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_v2.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m /kaggle/input/all-minilm-l6-v2/gguf/q4_k_m/1/all-MiniLM-L6-v2-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:36:05.735596Z","iopub.execute_input":"2024-09-26T09:36:05.735931Z","iopub.status.idle":"2024-09-26T09:36:08.697924Z","shell.execute_reply.started":"2024-09-26T09:36:05.735899Z","shell.execute_reply":"2024-09-26T09:36:08.696695Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!tail -6 ppl_L6_fp16.log\n!tail -6 ppl_Q4_v2.log\n!tail -6 ppl_Q4.log","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:36:08.700932Z","iopub.execute_input":"2024-09-26T09:36:08.702060Z","iopub.status.idle":"2024-09-26T09:36:11.663307Z","shell.execute_reply.started":"2024-09-26T09:36:08.702013Z","shell.execute_reply":"2024-09-26T09:36:11.661960Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/bin/bash: line 1: /kaggle/working/llama.cpp/llama-perplexity: Permission denied\n/bin/bash: line 1: /kaggle/working/llama.cpp/llama-perplexity: Permission denied\n/bin/bash: line 1: /kaggle/working/llama.cpp/llama-perplexity: Permission denied\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Benchmarking the Inference Throughput and Memory Consumption\nLlama-bench shows performance and resource consumption. There are three types of test\n\n- Prompt processing (pp): processing a prompt in batches (-p)\n- Text generation (tg): generating a sequence of tokens (-n)\n- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\nIn the following test, we run text generation and prompt processing","metadata":{}},{"cell_type":"code","source":"!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/all-minilm-l6-v2/gguf/fp16/1/all-MiniLM-L6-v2.gguf -m /kaggle/input/all-minilm-l6-v2/gguf/q4_k_m/1/all-MiniLM-L6-v2-Q4_K_M-v2.gguf -m /kaggle/input/all-minilm-l6-v2/gguf/q4_k_m/1/all-MiniLM-L6-v2-Q4_K_M.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:36:11.665432Z","iopub.execute_input":"2024-09-26T09:36:11.665839Z","iopub.status.idle":"2024-09-26T09:36:12.696937Z","shell.execute_reply.started":"2024-09-26T09:36:11.665795Z","shell.execute_reply":"2024-09-26T09:36:12.695826Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/bin/bash: line 1: /kaggle/working/llama.cpp/llama-bench: Permission denied\n","output_type":"stream"}]}]}