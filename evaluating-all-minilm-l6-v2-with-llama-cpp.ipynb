{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":198344523,"sourceType":"kernelVersion"},{"sourceId":120929,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":101726,"modelId":125945},{"sourceId":120930,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":101727,"modelId":125945}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nThis note evaluate smolLM-135M instruct model with llama.cpp\n\n# Use pre-complie llama.cpp","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/evaluating-smollm-135m-instruct-gguf-with-llamacpp/llama.cpp /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:26:04.420863Z","iopub.execute_input":"2024-09-26T09:26:04.421160Z","iopub.status.idle":"2024-09-26T09:26:57.870668Z","shell.execute_reply.started":"2024-09-26T09:26:04.421128Z","shell.execute_reply":"2024-09-26T09:26:57.869452Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!chmod +x llama.cpp\n%cd llama.cpp\n!pip install --force-reinstall -r requirements.txt > pip_install.log 2>&1\n!tail pip_install.log","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:30:28.540963Z","iopub.execute_input":"2024-09-26T09:30:28.541353Z","iopub.status.idle":"2024-09-26T09:30:37.340233Z","shell.execute_reply.started":"2024-09-26T09:30:28.541315Z","shell.execute_reply":"2024-09-26T09:30:37.338753Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"chmod: cannot access 'llama.cpp': No such file or directory\n[Errno 2] No such file or directory: 'llama.cpp'\n/kaggle/working/llama.cpp\n^C\n","output_type":"stream"},{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Run perplexity\nPerplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens. Formally, the perplexity of LLMs is the exponentiated average negative log-likelihood, which means perplexity smaller is better\n\nPerplexity benchmark shows how accurate the model is","metadata":{}},{"cell_type":"code","source":"!/kaggle/working/llama.cpp/llama-perplexity -m /kaggle/input/all-minilm-l6-v2/gguf/fp16/1/all-MiniLM-L6-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_L6_fp16.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m /kaggle/input/all-minilm-l6-v2/gguf/q4_k_m/1/all-MiniLM-L6-v2-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_v2.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m /kaggle/input/all-minilm-l6-v2/gguf/q4_k_m/1/all-MiniLM-L6-v2-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:29:32.962199Z","iopub.execute_input":"2024-09-26T09:29:32.962614Z","iopub.status.idle":"2024-09-26T09:29:35.938149Z","shell.execute_reply.started":"2024-09-26T09:29:32.962567Z","shell.execute_reply":"2024-09-26T09:29:35.937010Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!tail -6 ppl_L6_fp16.log\n!tail -6 ppl_Q4_v2.log\n!tail -6 ppl_Q4.log","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:29:35.939742Z","iopub.execute_input":"2024-09-26T09:29:35.940078Z","iopub.status.idle":"2024-09-26T09:29:38.991067Z","shell.execute_reply.started":"2024-09-26T09:29:35.940043Z","shell.execute_reply":"2024-09-26T09:29:38.989893Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/bin/bash: line 1: /kaggle/working/llama.cpp/llama-perplexity: Permission denied\n/bin/bash: line 1: /kaggle/working/llama.cpp/llama-perplexity: Permission denied\n/bin/bash: line 1: /kaggle/working/llama.cpp/llama-perplexity: Permission denied\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Benchmarking the Inference Throughput and Memory Consumption\nLlama-bench shows performance and resource consumption. There are three types of test\n\n- Prompt processing (pp): processing a prompt in batches (-p)\n- Text generation (tg): generating a sequence of tokens (-n)\n- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\nIn the following test, we run text generation and prompt processing","metadata":{}},{"cell_type":"code","source":"!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/all-minilm-l6-v2/gguf/fp16/1/all-MiniLM-L6-v2.gguf -m /kaggle/input/all-minilm-l6-v2/gguf/q4_k_m/1/all-MiniLM-L6-v2-Q4_K_M-v2.gguf -m /kaggle/input/all-minilm-l6-v2/gguf/q4_k_m/1/all-MiniLM-L6-v2-Q4_K_M.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:29:38.993389Z","iopub.execute_input":"2024-09-26T09:29:38.993758Z","iopub.status.idle":"2024-09-26T09:29:39.994173Z","shell.execute_reply.started":"2024-09-26T09:29:38.993721Z","shell.execute_reply":"2024-09-26T09:29:39.992983Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/bin/bash: line 1: /kaggle/working/llama.cpp/llama-bench: Permission denied\n","output_type":"stream"}]}]}