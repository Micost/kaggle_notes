{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We use llama_cpp to evaluate LLM followed by this article:\n\nhttps://towardsdatascience.com/gguf-quantization-with-imatrix-and-k-quantization-to-run-llms-on-your-cpu-02356b531926\n\nOnly use the evaluation part.\n\n# Install llama cpp","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp\n%cd llama.cpp\n!export GGML_CUDA=1 \n!make > make.log 2>&1","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:15:22.441054Z","iopub.execute_input":"2024-09-23T16:15:22.441604Z","iopub.status.idle":"2024-09-23T16:21:42.202332Z","shell.execute_reply.started":"2024-09-23T16:15:22.441472Z","shell.execute_reply":"2024-09-23T16:21:42.201011Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 34581, done.\u001b[K\nremote: Counting objects: 100% (111/111), done.\u001b[K\nremote: Compressing objects: 100% (72/72), done.\u001b[K\nremote: Total 34581 (delta 46), reused 71 (delta 36), pack-reused 34470 (from 1)\u001b[K\nReceiving objects: 100% (34581/34581), 57.57 MiB | 26.58 MiB/s, done.\nResolving deltas: 100% (24992/24992), done.\n/kaggle/working/llama.cpp\n","output_type":"stream"}]},{"cell_type":"code","source":"!tail make.log","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:21:42.204756Z","iopub.execute_input":"2024-09-23T16:21:42.205173Z","iopub.status.idle":"2024-09-23T16:21:43.224299Z","shell.execute_reply.started":"2024-09-23T16:21:42.205128Z","shell.execute_reply":"2024-09-23T16:21:43.222995Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/cvector-generator/cvector-generator.cpp -o examples/cvector-generator/cvector-generator.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/llamafile/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/build-info.o common/json-schema-to-grammar.o examples/cvector-generator/cvector-generator.o -o llama-cvector-generator  \nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gen-docs/gen-docs.cpp -o examples/gen-docs/gen-docs.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/llamafile/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/build-info.o common/json-schema-to-grammar.o examples/gen-docs/gen-docs.o -o llama-gen-docs  \ncc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/deprecation-warning/deprecation-warning.cpp -o examples/deprecation-warning/deprecation-warning.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  examples/deprecation-warning/deprecation-warning.o -o main  \nNOTICE: The 'main' binary is deprecated. Please use 'llama-cli' instead.\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  examples/deprecation-warning/deprecation-warning.o -o server  \nNOTICE: The 'server' binary is deprecated. Please use 'llama-server' instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --force-reinstall -r requirements.txt > pip_install.log 2>&1\n!tail pip_install.log","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:21:43.225843Z","iopub.execute_input":"2024-09-23T16:21:43.226172Z","iopub.status.idle":"2024-09-23T16:23:25.187137Z","shell.execute_reply.started":"2024-09-23T16:21:43.226138Z","shell.execute_reply":"2024-09-23T16:23:25.186024Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"pointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ns3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2024.9.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\nydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-2.1.5 certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.1 fsspec-2024.6.1 gguf-0.10.0 huggingface-hub-0.25.1 idna-3.10 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 packaging-24.1 protobuf-4.25.3 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 sentencepiece-0.2.0 sympy-1.13.3 tokenizers-0.19.1 torch-2.2.2+cpu tqdm-4.66.5 transformers-4.44.2 typing-extensions-4.12.2 urllib3-2.2.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Download gguf model\n\n## Prepare huggingface token","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n\nuser_secrets = UserSecretsClient()\n\nos.environ[\"HF_TOKEN\"]=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(os.environ[\"HF_TOKEN\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:23:25.190290Z","iopub.execute_input":"2024-09-23T16:23:25.191018Z","iopub.status.idle":"2024-09-23T16:23:26.032294Z","shell.execute_reply.started":"2024-09-23T16:23:25.190981Z","shell.execute_reply":"2024-09-23T16:23:26.031352Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Download gguf model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\nmodel_name = \"google/gemma-2-2b-it\" # the model we want to quantize\nmethods = ['Q4_K_S','Q4_K_M'] #the methods to be used for quantization\nbase_model = \"./original_model_gemma2-2b/\" # where the FP16 GGUF model will be stored\nquantized_path = \"./quantized_model_gemma2-2b/\" #where the quantized GGUF model will be stored\noriginal_model = quantized_path + 'FP16.gguf'\n\nsnapshot_download(repo_id=model_name, local_dir=base_model , local_dir_use_symlinks=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:23:26.034167Z","iopub.execute_input":"2024-09-23T16:23:26.034640Z","iopub.status.idle":"2024-09-23T16:23:48.085732Z","shell.execute_reply.started":"2024-09-23T16:23:26.034586Z","shell.execute_reply":"2024-09-23T16:23:48.084797Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2a7d3a6e3d6484cacd0473610e2cfd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c6d951098804fe194a416f76bf6148c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fd9ff91e03e40d2af7208b05aafa5a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a34c4c1ec8814454bbc95f18739d96b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5b7245c2c7245bfbf352445015f79f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a444070e8c4a49c699d1e5e399ccc1a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d81e521466544f51ad2c114c3c50c124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a0e807c8fd8416c93e4e3c3c033326c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/29.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42242a1763a54a4181d44d7767ebbd2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74a358b37cbe4030baa4efeef0e94564"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3239fc91ba3944298a55761a9a40fe53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f634d595320546d2ae54605c6a0cd68b"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/llama.cpp/original_model_gemma2-2b'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Convert model to gguf ","metadata":{}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/llama.cpp/quantized_model_gemma2-2b/\n!python convert_hf_to_gguf.py \"/kaggle/working/llama.cpp/original_model_gemma2-2b/\" --outfile \"/kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf\"","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:23:48.087107Z","iopub.execute_input":"2024-09-23T16:23:48.087485Z","iopub.status.idle":"2024-09-23T16:24:20.274977Z","shell.execute_reply.started":"2024-09-23T16:23:48.087440Z","shell.execute_reply":"2024-09-23T16:24:20.273573Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Writing: 100%|███████████████████████████| 5.23G/5.23G [00:22<00:00, 237Mbyte/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Get wiki text as dataset","metadata":{}},{"cell_type":"code","source":"!wget https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\n!gunzip en.txt.gz\n!head -n 10000 en.txt > en-h10000.txt\n!sh scripts/get-wikitext-2.sh","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:24:20.276859Z","iopub.execute_input":"2024-09-23T16:24:20.277228Z","iopub.status.idle":"2024-09-23T16:25:05.913567Z","shell.execute_reply.started":"2024-09-23T16:24:20.277194Z","shell.execute_reply":"2024-09-23T16:25:05.912344Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"--2024-09-23 16:24:21--  https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\nResolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.19, 86.50.254.18\nConnecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.19|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 532958396 (508M) [application/gzip]\nSaving to: 'en.txt.gz'\n\nen.txt.gz           100%[===================>] 508.27M  18.6MB/s    in 27s     \n\n2024-09-23 16:24:49 (18.7 MB/s) - 'en.txt.gz' saved [532958396/532958396]\n\n--2024-09-23 16:25:05--  https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip\nResolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.59, 3.165.160.12, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs-us-1.huggingface.co/repos/c6/78/c67802fcd48fa6f6a86773410b21cc6db1c5c546b20683b6c30b95f327a66922/ef7edb566e3e2b2d31b29c1fdb0c89a4cc683597484c3dc2517919c615435a11?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27wikitext-2-raw-v1.zip%3B+filename%3D%22wikitext-2-raw-v1.zip%22%3B&response-content-type=application%2Fzip&Expires=1727367905&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzM2NzkwNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2M2Lzc4L2M2NzgwMmZjZDQ4ZmE2ZjZhODY3NzM0MTBiMjFjYzZkYjFjNWM1NDZiMjA2ODNiNmMzMGI5NWYzMjdhNjY5MjIvZWY3ZWRiNTY2ZTNlMmIyZDMxYjI5YzFmZGIwYzg5YTRjYzY4MzU5NzQ4NGMzZGMyNTE3OTE5YzYxNTQzNWExMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=jD9FWt8wkrcoXFOxl6fZq37koxRoOFX99FshTijFR60rPb34SIuWkJXlt%7EUo9ZatFeLF34OrlIirQMcRctUPc6ybslBwkP7aV1gcoQTg7MKrnY9dxpV%7EYWKR3w8vese5s9axxA1bPXjvWty4yR6nwK5XXUv3l8Zuy-yIckqFA2qSiRsxUeMiATeKDHGiFvsgxp8GvScp-nfgZkDmLpbcQe%7ElnGZGzyVTfp4WYb-C%7Exzd0gKU1jGWEFBJBSKKcybpaef4TFbU0xmx-ik4RqZN2zzrqoXquMJjAbZUeQ6%7EcYKlBLt7EiCOAOKr6jRhNXlFd485st7%7EuEH7Y0tvmswoMw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n--2024-09-23 16:25:05--  https://cdn-lfs-us-1.huggingface.co/repos/c6/78/c67802fcd48fa6f6a86773410b21cc6db1c5c546b20683b6c30b95f327a66922/ef7edb566e3e2b2d31b29c1fdb0c89a4cc683597484c3dc2517919c615435a11?response-content-disposition=inline%3B+filename*%3DUTF-8''wikitext-2-raw-v1.zip%3B+filename%3D%22wikitext-2-raw-v1.zip%22%3B&response-content-type=application%2Fzip&Expires=1727367905&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzM2NzkwNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2M2Lzc4L2M2NzgwMmZjZDQ4ZmE2ZjZhODY3NzM0MTBiMjFjYzZkYjFjNWM1NDZiMjA2ODNiNmMzMGI5NWYzMjdhNjY5MjIvZWY3ZWRiNTY2ZTNlMmIyZDMxYjI5YzFmZGIwYzg5YTRjYzY4MzU5NzQ4NGMzZGMyNTE3OTE5YzYxNTQzNWExMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=jD9FWt8wkrcoXFOxl6fZq37koxRoOFX99FshTijFR60rPb34SIuWkJXlt~Uo9ZatFeLF34OrlIirQMcRctUPc6ybslBwkP7aV1gcoQTg7MKrnY9dxpV~YWKR3w8vese5s9axxA1bPXjvWty4yR6nwK5XXUv3l8Zuy-yIckqFA2qSiRsxUeMiATeKDHGiFvsgxp8GvScp-nfgZkDmLpbcQe~lnGZGzyVTfp4WYb-C~xzd0gKU1jGWEFBJBSKKcybpaef4TFbU0xmx-ik4RqZN2zzrqoXquMJjAbZUeQ6~cYKlBLt7EiCOAOKr6jRhNXlFd485st7~uEH7Y0tvmswoMw__&Key-Pair-Id=K24J24Z295AEI9\nResolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.163.189.20, 3.163.189.28, 3.163.189.127, ...\nConnecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.163.189.20|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4721645 (4.5M) [application/zip]\nSaving to: 'wikitext-2-raw-v1.zip'\n\nwikitext-2-raw-v1.z 100%[===================>]   4.50M  --.-KB/s    in 0.08s   \n\n2024-09-23 16:25:05 (54.3 MB/s) - 'wikitext-2-raw-v1.zip' saved [4721645/4721645]\n\nArchive:  wikitext-2-raw-v1.zip\n   creating: wikitext-2-raw/\n  inflating: wikitext-2-raw/wiki.test.raw  \n  inflating: wikitext-2-raw/wiki.valid.raw  \n  inflating: wikitext-2-raw/wiki.train.raw  \nUsage:\n\n  ./llama-perplexity -m model.gguf -f wikitext-2-raw/wiki.test.raw [other params]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Benchmarking the Perplexity \n\nPerplexity can be used to compare the models before and after quantization or other method. Here is a explaination of why we could not compare different models by benchmarking perplexity\n\nhttps://thesalt.substack.com/p/why-cant-we-compare-the-perplexity\n","metadata":{}},{"cell_type":"code","source":"!./llama-perplexity -m /kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf -f wikitext-2-raw/wiki.test.raw --chunks 16","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:25:05.915075Z","iopub.execute_input":"2024-09-23T16:25:05.915429Z","iopub.status.idle":"2024-09-23T16:33:57.129669Z","shell.execute_reply.started":"2024-09-23T16:25:05.915393Z","shell.execute_reply":"2024-09-23T16:33:57.128569Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"build: 3810 (1d48e98e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nllama_model_loader: loaded meta data with 38 key-value pairs and 288 tensors from /kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Original_Model_Gemma2 2b\nllama_model_loader: - kv   3:                           general.basename str              = original_model_gemma2\nllama_model_loader: - kv   4:                         general.size_label str              = 2B\nllama_model_loader: - kv   5:                            general.license str              = gemma\nllama_model_loader: - kv   6:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   7:                  general.base_model.0.name str              = Gemma 2 2b\nllama_model_loader: - kv   8:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-2-2b\nllama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\nllama_model_loader: - kv  11:                      gemma2.context_length u32              = 8192\nllama_model_loader: - kv  12:                    gemma2.embedding_length u32              = 2304\nllama_model_loader: - kv  13:                         gemma2.block_count u32              = 26\nllama_model_loader: - kv  14:                 gemma2.feed_forward_length u32              = 9216\nllama_model_loader: - kv  15:                gemma2.attention.head_count u32              = 8\nllama_model_loader: - kv  16:             gemma2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  17:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma2.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma2.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                          general.file_type u32              = 1\nllama_model_loader: - kv  21:              gemma2.attn_logit_softcapping f32              = 50.000000\nllama_model_loader: - kv  22:             gemma2.final_logit_softcapping f32              = 30.000000\nllama_model_loader: - kv  23:            gemma2.attention.sliding_window u32              = 4096\nllama_model_loader: - kv  24:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  27:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  35:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\nllama_model_loader: - kv  36:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  37:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  105 tensors\nllama_model_loader: - type  f16:  183 tensors\nllm_load_vocab: special tokens cache size = 249\nllm_load_vocab: token to piece cache size = 1.6014 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = gemma2\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 256000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 2304\nllm_load_print_meta: n_layer          = 26\nllm_load_print_meta: n_head           = 8\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 256\nllm_load_print_meta: n_swa            = 4096\nllm_load_print_meta: n_embd_head_k    = 256\nllm_load_print_meta: n_embd_head_v    = 256\nllm_load_print_meta: n_gqa            = 2\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 9216\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 2B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 2.61 B\nllm_load_print_meta: model size       = 4.87 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Original_Model_Gemma2 2b\nllm_load_print_meta: BOS token        = 2 '<bos>'\nllm_load_print_meta: EOS token        = 1 '<eos>'\nllm_load_print_meta: UNK token        = 3 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<pad>'\nllm_load_print_meta: LF token         = 227 '<0x0A>'\nllm_load_print_meta: EOT token        = 107 '<end_of_turn>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.13 MiB\nllm_load_tensors:        CPU buffer size =  4986.92 MiB\n..................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =   208.00 MiB\nllama_new_context_with_model: KV self size  =  208.00 MiB, K (f16):  104.00 MiB, V (f16):  104.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     3.91 MiB\nllama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\nllama_new_context_with_model: graph nodes  = 1050\nllama_new_context_with_model: graph splits = 1\nllama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 4 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nperplexity: tokenizing the input ..\nperplexity: tokenization took 1507.1 ms\nperplexity: calculating perplexity over 16 chunks, n_ctx=512, batch_size=2048, n_seq=4\nperplexity: 132.14 seconds per pass - ETA 8.80 minutes\n[1]7.8101,[2]10.2032,[3]10.0413,[4]10.1833,[5]10.2147,[6]11.2205,[7]11.6780,[8]12.2438,[9]13.6003,[10]14.1319,[11]14.2673,[12]14.8171,[13]16.1681,[14]15.1809,[15]14.7036,[16]14.3487,\nFinal estimate: PPL = 14.3487 +/- 0.70006\n\nllama_perf_context_print:        load time =    1250.51 ms\nllama_perf_context_print: prompt eval time =  521990.36 ms /  8192 tokens (   63.72 ms per token,    15.69 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =  528416.31 ms /  8193 tokens\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Benchmarking the Inference Throughput and Memory Consumption ","metadata":{}},{"cell_type":"code","source":"!./llama-bench -m /kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf -n 16 -mg 1","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:36:53.739210Z","iopub.execute_input":"2024-09-23T16:36:53.739671Z","iopub.status.idle":"2024-09-23T16:39:47.910607Z","shell.execute_reply.started":"2024-09-23T16:36:53.739630Z","shell.execute_reply":"2024-09-23T16:39:47.909467Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"| model                          |       size |     params | backend    | threads |   main_gpu |          test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ---------: | ------------: | -------------------: |\n| gemma2 2B F16                  |   5.97 GiB |     3.20 B | CPU        |       2 |          1 |         pp512 |         20.42 ± 0.16 |\n| gemma2 2B F16                  |   5.97 GiB |     3.20 B | CPU        |       2 |          1 |          tg16 |          3.68 ± 0.13 |\n\nbuild: 1d48e98e (3810)\n","output_type":"stream"}]}]}