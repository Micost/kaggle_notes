{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We use llama_cpp to evaluate LLM followed by this article:\n\nhttps://towardsdatascience.com/gguf-quantization-with-imatrix-and-k-quantization-to-run-llms-on-your-cpu-02356b531926\n\nOnly use the evaluation part.\n\n# Install llama cpp","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp\n%cd llama.cpp\n# !export GGML_CUDA=1 \n!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ \n!make GGML_CUDA=1 CUDA_PATH=/usr/local/nvidia  > make.log 2>&1","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:26:21.739553Z","iopub.execute_input":"2024-09-24T04:26:21.740493Z","iopub.status.idle":"2024-09-24T04:48:17.389735Z","shell.execute_reply.started":"2024-09-24T04:26:21.740431Z","shell.execute_reply":"2024-09-24T04:48:17.388178Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 34606, done.\u001b[K\nremote: Counting objects: 100% (133/133), done.\u001b[K\nremote: Compressing objects: 100% (85/85), done.\u001b[K\nremote: Total 34606 (delta 63), reused 91 (delta 45), pack-reused 34473 (from 1)\u001b[K\nReceiving objects: 100% (34606/34606), 57.70 MiB | 24.65 MiB/s, done.\nResolving deltas: 100% (25012/25012), done.\n/kaggle/working/llama.cpp\n","output_type":"stream"}]},{"cell_type":"code","source":"!tail make.log\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:48:17.392819Z","iopub.execute_input":"2024-09-24T04:48:17.393329Z","iopub.status.idle":"2024-09-24T04:48:18.386566Z","shell.execute_reply.started":"2024-09-24T04:48:17.393272Z","shell.execute_reply":"2024-09-24T04:48:18.385626Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -c examples/cvector-generator/cvector-generator.cpp -o examples/cvector-generator/cvector-generator.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  ggml/src/llamafile/sgemm.o ggml/src/ggml-cuda.o ggml/src/ggml-cuda/acc.o ggml/src/ggml-cuda/arange.o ggml/src/ggml-cuda/argsort.o ggml/src/ggml-cuda/binbcast.o ggml/src/ggml-cuda/clamp.o ggml/src/ggml-cuda/concat.o ggml/src/ggml-cuda/conv-transpose-1d.o ggml/src/ggml-cuda/convert.o ggml/src/ggml-cuda/cpy.o ggml/src/ggml-cuda/cross-entropy-loss.o ggml/src/ggml-cuda/diagmask.o ggml/src/ggml-cuda/dmmv.o ggml/src/ggml-cuda/fattn-tile-f16.o ggml/src/ggml-cuda/fattn-tile-f32.o ggml/src/ggml-cuda/fattn.o ggml/src/ggml-cuda/getrows.o ggml/src/ggml-cuda/im2col.o ggml/src/ggml-cuda/mmq.o ggml/src/ggml-cuda/mmvq.o ggml/src/ggml-cuda/norm.o ggml/src/ggml-cuda/opt-step-adamw.o ggml/src/ggml-cuda/out-prod.o ggml/src/ggml-cuda/pad.o ggml/src/ggml-cuda/pool2d.o ggml/src/ggml-cuda/quantize.o ggml/src/ggml-cuda/rope.o ggml/src/ggml-cuda/rwkv-wkv.o ggml/src/ggml-cuda/scale.o ggml/src/ggml-cuda/softmax.o ggml/src/ggml-cuda/sum.o ggml/src/ggml-cuda/sumrows.o ggml/src/ggml-cuda/tsembd.o ggml/src/ggml-cuda/unary.o ggml/src/ggml-cuda/upscale.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/build-info.o common/json-schema-to-grammar.o examples/cvector-generator/cvector-generator.o -o llama-cvector-generator -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -c examples/gen-docs/gen-docs.cpp -o examples/gen-docs/gen-docs.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  ggml/src/llamafile/sgemm.o ggml/src/ggml-cuda.o ggml/src/ggml-cuda/acc.o ggml/src/ggml-cuda/arange.o ggml/src/ggml-cuda/argsort.o ggml/src/ggml-cuda/binbcast.o ggml/src/ggml-cuda/clamp.o ggml/src/ggml-cuda/concat.o ggml/src/ggml-cuda/conv-transpose-1d.o ggml/src/ggml-cuda/convert.o ggml/src/ggml-cuda/cpy.o ggml/src/ggml-cuda/cross-entropy-loss.o ggml/src/ggml-cuda/diagmask.o ggml/src/ggml-cuda/dmmv.o ggml/src/ggml-cuda/fattn-tile-f16.o ggml/src/ggml-cuda/fattn-tile-f32.o ggml/src/ggml-cuda/fattn.o ggml/src/ggml-cuda/getrows.o ggml/src/ggml-cuda/im2col.o ggml/src/ggml-cuda/mmq.o ggml/src/ggml-cuda/mmvq.o ggml/src/ggml-cuda/norm.o ggml/src/ggml-cuda/opt-step-adamw.o ggml/src/ggml-cuda/out-prod.o ggml/src/ggml-cuda/pad.o ggml/src/ggml-cuda/pool2d.o ggml/src/ggml-cuda/quantize.o ggml/src/ggml-cuda/rope.o ggml/src/ggml-cuda/rwkv-wkv.o ggml/src/ggml-cuda/scale.o ggml/src/ggml-cuda/softmax.o ggml/src/ggml-cuda/sum.o ggml/src/ggml-cuda/sumrows.o ggml/src/ggml-cuda/tsembd.o ggml/src/ggml-cuda/unary.o ggml/src/ggml-cuda/upscale.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/build-info.o common/json-schema-to-grammar.o examples/gen-docs/gen-docs.o -o llama-gen-docs -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \ncc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -c examples/deprecation-warning/deprecation-warning.cpp -o examples/deprecation-warning/deprecation-warning.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  examples/deprecation-warning/deprecation-warning.o -o main -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \nNOTICE: The 'main' binary is deprecated. Please use 'llama-cli' instead.\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  examples/deprecation-warning/deprecation-warning.o -o server -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \nNOTICE: The 'server' binary is deprecated. Please use 'llama-server' instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --force-reinstall -r requirements.txt > pip_install.log 2>&1\n!tail pip_install.log","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:48:18.387939Z","iopub.execute_input":"2024-09-24T04:48:18.388277Z","iopub.status.idle":"2024-09-24T04:50:05.344820Z","shell.execute_reply.started":"2024-09-24T04:48:18.388241Z","shell.execute_reply":"2024-09-24T04:50:05.343685Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"pointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ns3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2024.9.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\nydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-2.1.5 certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.1 fsspec-2024.6.1 gguf-0.10.0 huggingface-hub-0.25.1 idna-3.10 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 packaging-24.1 protobuf-4.25.3 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 sentencepiece-0.2.0 sympy-1.13.3 tokenizers-0.19.1 torch-2.2.2+cpu tqdm-4.66.5 transformers-4.44.2 typing-extensions-4.12.2 urllib3-2.2.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Download gguf model\n\n## Prepare huggingface token","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n\nuser_secrets = UserSecretsClient()\n\nos.environ[\"HF_TOKEN\"]=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(os.environ[\"HF_TOKEN\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:50:05.346470Z","iopub.execute_input":"2024-09-24T04:50:05.346786Z","iopub.status.idle":"2024-09-24T04:50:06.026222Z","shell.execute_reply.started":"2024-09-24T04:50:05.346753Z","shell.execute_reply":"2024-09-24T04:50:06.025297Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Download gguf model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\nmodel_name = \"google/gemma-2-2b-it\" # the model we want to quantize\nmethods = ['Q4_K_S','Q4_K_M'] #the methods to be used for quantization\nbase_model = \"./original_model_gemma2-2b/\" # where the FP16 GGUF model will be stored\nquantized_path = \"./quantized_model_gemma2-2b/\" #where the quantized GGUF model will be stored\noriginal_model = quantized_path + 'FP16.gguf'\n\nsnapshot_download(repo_id=model_name, local_dir=base_model , local_dir_use_symlinks=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:50:06.028512Z","iopub.execute_input":"2024-09-24T04:50:06.028822Z","iopub.status.idle":"2024-09-24T04:50:39.437944Z","shell.execute_reply.started":"2024-09-24T04:50:06.028789Z","shell.execute_reply":"2024-09-24T04:50:39.437010Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0212f780e6c1452289f51b6e04052dec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f05412fc0a4c3d95bf4418b4586d5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f47332b9e0b4b4c82b6eb5e2d34627f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1a56d22e7b4c0484d83f0498abb88c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cada6ac281b4570b7fef13065409faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/29.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45f15fd70bf5485f8261de9050ec36fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edbda48845e1458ca89a3f1334ec7b88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f52f2492806e4fd89225e8dfc89cd280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e857b49e66490a9cb98df37368f32a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3644a7ca3bbc430e9675934629291ab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f8e75b1c84542d2980387636e7971fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f45a19e54bc44578a4a9c7293d8b59c"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/llama.cpp/original_model_gemma2-2b'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Convert model to gguf ","metadata":{}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/llama.cpp/quantized_model_gemma2-2b/\n!python convert_hf_to_gguf.py \"/kaggle/working/llama.cpp/original_model_gemma2-2b/\" --outfile \"/kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf\"","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:50:39.439125Z","iopub.execute_input":"2024-09-24T04:50:39.439480Z","iopub.status.idle":"2024-09-24T04:51:11.587689Z","shell.execute_reply.started":"2024-09-24T04:50:39.439446Z","shell.execute_reply":"2024-09-24T04:51:11.586557Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing: 100%|███████████████████████████| 5.23G/5.23G [00:22<00:00, 236Mbyte/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Get wiki text as dataset","metadata":{}},{"cell_type":"code","source":"!wget https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\n!gunzip en.txt.gz\n!head -n 10000 en.txt > en-h10000.txt\n!sh scripts/get-wikitext-2.sh","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:51:11.589293Z","iopub.execute_input":"2024-09-24T04:51:11.590209Z","iopub.status.idle":"2024-09-24T04:52:00.587234Z","shell.execute_reply.started":"2024-09-24T04:51:11.590139Z","shell.execute_reply":"2024-09-24T04:52:00.586118Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"--2024-09-24 04:51:12--  https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\nResolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\nConnecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 532958396 (508M) [application/gzip]\nSaving to: 'en.txt.gz'\n\nen.txt.gz           100%[===================>] 508.27M  15.7MB/s    in 31s     \n\n2024-09-24 04:51:44 (16.6 MB/s) - 'en.txt.gz' saved [532958396/532958396]\n\n--2024-09-24 04:51:59--  https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip\nResolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.11, 3.165.160.61, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs-us-1.huggingface.co/repos/c6/78/c67802fcd48fa6f6a86773410b21cc6db1c5c546b20683b6c30b95f327a66922/ef7edb566e3e2b2d31b29c1fdb0c89a4cc683597484c3dc2517919c615435a11?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27wikitext-2-raw-v1.zip%3B+filename%3D%22wikitext-2-raw-v1.zip%22%3B&response-content-type=application%2Fzip&Expires=1727411311&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzQxMTMxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2M2Lzc4L2M2NzgwMmZjZDQ4ZmE2ZjZhODY3NzM0MTBiMjFjYzZkYjFjNWM1NDZiMjA2ODNiNmMzMGI5NWYzMjdhNjY5MjIvZWY3ZWRiNTY2ZTNlMmIyZDMxYjI5YzFmZGIwYzg5YTRjYzY4MzU5NzQ4NGMzZGMyNTE3OTE5YzYxNTQzNWExMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=mii2MtoFmeujJ5jK80ax%7EUM9fsrqe56pUj0U%7ESQkJOZfj6VjpllfDPxUL2I0dzEs8F23pinxgFTdFTjNS9UWuCRk8eY6cYGu7OUjWqaJjEU%7EEi-9hIfRY13jSyKhF8iVDySAP6bbYuq9f3%7EPtLzhWoI8RspV0HhkJzL5GC%7EPNfo6hvRDnMYE2l9wAQK9HbgAmZ9gnG4YZqEKe7n7W50fueakKDSMySKLjkWT2bCggdcj2Lgy0Bqucd51X7zhUGWgtgYWp9UTPNENPZoKJsAOvkeCYmcYDLyr53CLCjTX46XmypZiyvb9KbZD3Q7H6l1IUrr6T095Fg7O5xEM16h9pQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n--2024-09-24 04:52:00--  https://cdn-lfs-us-1.huggingface.co/repos/c6/78/c67802fcd48fa6f6a86773410b21cc6db1c5c546b20683b6c30b95f327a66922/ef7edb566e3e2b2d31b29c1fdb0c89a4cc683597484c3dc2517919c615435a11?response-content-disposition=inline%3B+filename*%3DUTF-8''wikitext-2-raw-v1.zip%3B+filename%3D%22wikitext-2-raw-v1.zip%22%3B&response-content-type=application%2Fzip&Expires=1727411311&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzQxMTMxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2M2Lzc4L2M2NzgwMmZjZDQ4ZmE2ZjZhODY3NzM0MTBiMjFjYzZkYjFjNWM1NDZiMjA2ODNiNmMzMGI5NWYzMjdhNjY5MjIvZWY3ZWRiNTY2ZTNlMmIyZDMxYjI5YzFmZGIwYzg5YTRjYzY4MzU5NzQ4NGMzZGMyNTE3OTE5YzYxNTQzNWExMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=mii2MtoFmeujJ5jK80ax~UM9fsrqe56pUj0U~SQkJOZfj6VjpllfDPxUL2I0dzEs8F23pinxgFTdFTjNS9UWuCRk8eY6cYGu7OUjWqaJjEU~Ei-9hIfRY13jSyKhF8iVDySAP6bbYuq9f3~PtLzhWoI8RspV0HhkJzL5GC~PNfo6hvRDnMYE2l9wAQK9HbgAmZ9gnG4YZqEKe7n7W50fueakKDSMySKLjkWT2bCggdcj2Lgy0Bqucd51X7zhUGWgtgYWp9UTPNENPZoKJsAOvkeCYmcYDLyr53CLCjTX46XmypZiyvb9KbZD3Q7H6l1IUrr6T095Fg7O5xEM16h9pQ__&Key-Pair-Id=K24J24Z295AEI9\nResolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.163.189.28, 3.163.189.91, 3.163.189.127, ...\nConnecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.163.189.28|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4721645 (4.5M) [application/zip]\nSaving to: 'wikitext-2-raw-v1.zip'\n\nwikitext-2-raw-v1.z 100%[===================>]   4.50M  --.-KB/s    in 0.07s   \n\n2024-09-24 04:52:00 (68.6 MB/s) - 'wikitext-2-raw-v1.zip' saved [4721645/4721645]\n\nArchive:  wikitext-2-raw-v1.zip\n   creating: wikitext-2-raw/\n  inflating: wikitext-2-raw/wiki.test.raw  \n  inflating: wikitext-2-raw/wiki.valid.raw  \n  inflating: wikitext-2-raw/wiki.train.raw  \nUsage:\n\n  ./llama-perplexity -m model.gguf -f wikitext-2-raw/wiki.test.raw [other params]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Benchmarking the Perplexity \n\nPerplexity can be used to compare the models before and after quantization or other method. Here is a explaination of why we could not compare different models by benchmarking perplexity\n\nhttps://thesalt.substack.com/p/why-cant-we-compare-the-perplexity\n","metadata":{}},{"cell_type":"code","source":"!./llama-perplexity -m /kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf -f wikitext-2-raw/wiki.test.raw --chunks 16","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:52:00.588881Z","iopub.execute_input":"2024-09-24T04:52:00.589258Z","iopub.status.idle":"2024-09-24T04:52:38.128005Z","shell.execute_reply.started":"2024-09-24T04:52:00.589220Z","shell.execute_reply":"2024-09-24T04:52:38.126891Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"build: 3814 (c087b6f1) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nllama_model_loader: loaded meta data with 38 key-value pairs and 288 tensors from /kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Original_Model_Gemma2 2b\nllama_model_loader: - kv   3:                           general.basename str              = original_model_gemma2\nllama_model_loader: - kv   4:                         general.size_label str              = 2B\nllama_model_loader: - kv   5:                            general.license str              = gemma\nllama_model_loader: - kv   6:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   7:                  general.base_model.0.name str              = Gemma 2 2b\nllama_model_loader: - kv   8:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-2-2b\nllama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\nllama_model_loader: - kv  11:                      gemma2.context_length u32              = 8192\nllama_model_loader: - kv  12:                    gemma2.embedding_length u32              = 2304\nllama_model_loader: - kv  13:                         gemma2.block_count u32              = 26\nllama_model_loader: - kv  14:                 gemma2.feed_forward_length u32              = 9216\nllama_model_loader: - kv  15:                gemma2.attention.head_count u32              = 8\nllama_model_loader: - kv  16:             gemma2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  17:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma2.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma2.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                          general.file_type u32              = 1\nllama_model_loader: - kv  21:              gemma2.attn_logit_softcapping f32              = 50.000000\nllama_model_loader: - kv  22:             gemma2.final_logit_softcapping f32              = 30.000000\nllama_model_loader: - kv  23:            gemma2.attention.sliding_window u32              = 4096\nllama_model_loader: - kv  24:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  27:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  35:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\nllama_model_loader: - kv  36:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  37:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  105 tensors\nllama_model_loader: - type  f16:  183 tensors\nllm_load_vocab: special tokens cache size = 249\nllm_load_vocab: token to piece cache size = 1.6014 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = gemma2\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 256000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 2304\nllm_load_print_meta: n_layer          = 26\nllm_load_print_meta: n_head           = 8\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 256\nllm_load_print_meta: n_swa            = 4096\nllm_load_print_meta: n_embd_head_k    = 256\nllm_load_print_meta: n_embd_head_v    = 256\nllm_load_print_meta: n_gqa            = 2\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 9216\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 2B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 2.61 B\nllm_load_print_meta: model size       = 4.87 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Original_Model_Gemma2 2b\nllm_load_print_meta: BOS token        = 2 '<bos>'\nllm_load_print_meta: EOS token        = 1 '<eos>'\nllm_load_print_meta: UNK token        = 3 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<pad>'\nllm_load_print_meta: LF token         = 227 '<0x0A>'\nllm_load_print_meta: EOT token        = 107 '<end_of_turn>'\nllm_load_print_meta: max token length = 48\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.13 MiB\nllm_load_tensors: offloading 0 repeating layers to GPU\nllm_load_tensors: offloaded 0/27 layers to GPU\nllm_load_tensors:        CPU buffer size =  4986.92 MiB\n..................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =   208.00 MiB\nllama_new_context_with_model: KV self size  =  208.00 MiB, K (f16):  104.00 MiB, V (f16):  104.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     3.91 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1674.50 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    12.51 MiB\nllama_new_context_with_model: graph nodes  = 1050\nllama_new_context_with_model: graph splits = 342\nllama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 4 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nperplexity: tokenizing the input ..\nperplexity: tokenization took 1454.38 ms\nperplexity: calculating perplexity over 16 chunks, n_ctx=512, batch_size=2048, n_seq=4\nperplexity: 7.50 seconds per pass - ETA 0.48 minutes\n[1]7.8105,[2]10.2050,[3]10.0401,[4]10.1840,[5]10.2151,[6]11.2225,[7]11.6813,[8]12.2473,[9]13.6048,[10]14.1367,[11]14.2721,[12]14.8206,[13]16.1724,[14]15.1847,[15]14.7066,[16]14.3515,\nFinal estimate: PPL = 14.3515 +/- 0.70035\n\nllama_perf_context_print:        load time =    1540.54 ms\nllama_perf_context_print: prompt eval time =   27383.66 ms /  8192 tokens (    3.34 ms per token,   299.16 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =   33878.44 ms /  8193 tokens\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Benchmarking the Inference Throughput and Memory Consumption ","metadata":{}},{"cell_type":"code","source":"!./llama-bench -m /kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf -n 16 -mg 1","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:52:38.129446Z","iopub.execute_input":"2024-09-24T04:52:38.129787Z","iopub.status.idle":"2024-09-24T04:52:45.434278Z","shell.execute_reply.started":"2024-09-24T04:52:38.129751Z","shell.execute_reply":"2024-09-24T04:52:45.433245Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\n| model                          |       size |     params | backend    | ngl |   main_gpu |          test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------: | -------------------: |\n| gemma2 2B F16                  |   5.97 GiB |     3.20 B | CUDA       |  99 |          1 |         pp512 |       1462.50 ± 2.80 |\n| gemma2 2B F16                  |   5.97 GiB |     3.20 B | CUDA       |  99 |          1 |          tg16 |         47.64 ± 0.13 |\n\nbuild: c087b6f1 (3814)\n","output_type":"stream"}]}]}