{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tunning SmolLM-135M with simple calculation\n\nThis is a fine tuning notes about using simple calculation dataset.\n\n\n# Environment prepare","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2025-02-16T09:37:43.957203Z","iopub.execute_input":"2025-02-16T09:37:43.957417Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Token and API\n\nWe’ll be tracking the training process using the Weights & Biases and then saving the fine-tuned model on Hugging Face, and for that, we have to log in to both Hugging Face Hub and Weights & Biases using the API key.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune SmolLM-135M on simple calculation', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Base model\n\nSet the base model, dataset, and new model variable. ","metadata":{}},{"cell_type":"code","source":"base_model = \"HuggingFaceTB/SmolLM-135M-Instruct\"\ndataset_name = \"micost/db_operate\"\nnew_model = \"SmolLM-135M-db-operate\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Set the data type and attention implementation.","metadata":{}},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the model and tokenizer\n\nIn this part, we’ll load the model from Kaggle. However, due to memory constraints, we’re unable to load the full model. Therefore, we’re loading the model using 4-bit precision.\n\nOur goal in this project is to reduce memory usage and speed up the fine-tuning process.","metadata":{}},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adding the adapter to the layer\n\nFine-tuning the full model will take a lot of time, so to improve the training time, we’ll attach the adapter layer with a few parameters, making the entire process faster and more memory-efficient.","metadata":{}},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the dataset\n\nTo load and pre-process our dataset, we:\n\n1. Load the ruslanmv/ai-medical-chatbot dataset, shuffle it, and select only the top 1000 rows. This will significantly reduce the training time.\n\n2. Format the chat template to make it conversational. Combine the patient questions and doctor responses into a \"text\" column.\n\n3. Display a sample from the text column (the “text” column has a chat-like format with special tokens).","metadata":{}},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(5000)) # Only use 5000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"input\"]},\n               {\"role\": \"assistant\", \"content\": row[\"output\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\ndataset['text'][3]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Split the dataset into a training and validation set.","metadata":{}},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Complaining and training the model\n\nWe are setting the model hyperparameters so that we can run it on the Kaggle. You can learn about each hyperparameter by reading the Fine-Tuning Llama 2 tutorial.\n\nWe are fine-tuning the model for one epoch and logging the metrics using the Weights and Biases.","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We’ll now set up a supervised fine-tuning (SFT) trainer and provide a train and evaluation dataset, LoRA configuration, training argument, tokenizer, and model. We’re keeping the max_seq_length to 512 to avoid exceeding GPU memory during training.","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model evaluation\n\nWhen you finish the Weights & Biases session, it’ll generate the run history and summary.","metadata":{}},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\ndef extract_result(text):\n  pattern = r'\\{.*\"result\":\\s*\"(\\d+)\".*\\}'\n  match = re.search(pattern, text)\n  if match:\n    return {\"result\": match.group(1)}\n  else:\n    return None\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"{ \\\"A\\\": \\”970\\\", \\\"B\\\": \\\"34\\\", \\\"op\\\": \\\"-\\\" }\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n\nprint(extract_result(text.split(\"assistant\")[1]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\ndef calculator(a, b, op, tokenizer, model):\n  messages = [\n    {\n      \"role\": \"user\",\n      \"content\": f\"{{\\\"A\\\": \\\"{a}\\\", \\\"B\\\": \\\"{b}\\\", \\\"op\\\": \\\"{op}\\\"}}\"\n    }\n  ]\n\n  prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n  inputs = tokenizer(prompt, return_tensors='pt',padding=True, truncation=True).to(\"cuda\")\n  outputs = model.generate(**inputs, max_length=150, num_return_sequences=1)\n  text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  result_str = extract_result(text.split(\"assistant\")[1])\n  #result_dict = json.loads(result_str)\n  result_value = result_str[\"result\"]\n  return result_value\n\n# result = calculator(970, 34, \"-\", tokenizer, model)\nprint(result)  # Output: 936","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}