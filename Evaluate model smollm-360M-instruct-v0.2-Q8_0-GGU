{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-04T16:22:50.450833Z","iopub.execute_input":"2024-09-04T16:22:50.451402Z","iopub.status.idle":"2024-09-04T16:22:50.459876Z","shell.execute_reply.started":"2024-09-04T16:22:50.451353Z","shell.execute_reply":"2024-09-04T16:22:50.458537Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Overview\n\nEvaluate model HuggingFaceTB/smollm-360M-instruct-v0.2-Q8_0-GGUF by using https://github.com/EleutherAI/lm-evaluation-harness\n","metadata":{}},{"cell_type":"code","source":"# !pip install -U -q lm-eval==0.4.3","metadata":{"execution":{"iopub.status.busy":"2024-09-04T16:22:50.462265Z","iopub.execute_input":"2024-09-04T16:22:50.463469Z","iopub.status.idle":"2024-09-04T16:22:50.477606Z","shell.execute_reply.started":"2024-09-04T16:22:50.463410Z","shell.execute_reply":"2024-09-04T16:22:50.476246Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/EleutherAI/lm-evaluation-harness\n%cd lm-evaluation-harness\n!pip install -e .","metadata":{"execution":{"iopub.status.busy":"2024-09-04T16:22:50.478986Z","iopub.execute_input":"2024-09-04T16:22:50.479400Z","iopub.status.idle":"2024-09-04T16:23:51.981985Z","shell.execute_reply.started":"2024-09-04T16:22:50.479360Z","shell.execute_reply":"2024-09-04T16:23:51.980453Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Cloning into 'lm-evaluation-harness'...\nremote: Enumerating objects: 40183, done.\u001b[K\nremote: Counting objects: 100% (512/512), done.\u001b[K\nremote: Compressing objects: 100% (353/353), done.\u001b[K\nremote: Total 40183 (delta 247), reused 387 (delta 158), pack-reused 39671 (from 1)\u001b[K\nReceiving objects: 100% (40183/40183), 26.96 MiB | 15.61 MiB/s, done.\nResolving deltas: 100% (28176/28176), done.\n/kaggle/working/lm-evaluation-harness/lm-evaluation-harness\nObtaining file:///kaggle/working/lm-evaluation-harness/lm-evaluation-harness\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (0.33.0)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (0.4.2)\nRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (2.21.0)\nRequirement already satisfied: jsonlines in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (4.0.0)\nRequirement already satisfied: numexpr in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (2.10.1)\nRequirement already satisfied: peft>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (0.12.0)\nRequirement already satisfied: pybind11>=2.6.2 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (2.13.4)\nRequirement already satisfied: pytablewriter in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (1.2.0)\nRequirement already satisfied: rouge-score>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (0.1.2)\nRequirement already satisfied: sacrebleu>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (2.4.3)\nRequirement already satisfied: scikit-learn>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (1.2.2)\nRequirement already satisfied: sqlitedict in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (2.1.0)\nRequirement already satisfied: torch>=1.8 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (2.4.0+cpu)\nRequirement already satisfied: tqdm-multiprocess in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (0.0.11)\nRequirement already satisfied: transformers>=4.1 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (4.44.0)\nRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (0.23.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (0.3.8)\nRequirement already satisfied: word2number in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (1.1)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.3) (10.3.0)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->lm_eval==0.4.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->lm_eval==0.4.3) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->lm_eval==0.4.3) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->lm_eval==0.4.3) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->lm_eval==0.4.3) (0.24.6)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->lm_eval==0.4.3) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.3) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.3) (17.0.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.3) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.3) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.3) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.3) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.3) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.3) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.3) (3.9.5)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.3) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.3) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.3) (1.16.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.3) (2.10.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.3) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.3) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.3) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.3) (5.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.3) (1.14.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.3) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.3) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.3) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.3) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.3) (3.1.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.1->lm_eval==0.4.3) (0.19.1)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines->lm_eval==0.4.3) (23.2.0)\nRequirement already satisfied: setuptools>=38.3.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.3) (70.0.0)\nRequirement already satisfied: DataProperty<2,>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.3) (1.0.1)\nRequirement already satisfied: mbstrdecoder<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.3) (1.1.3)\nRequirement already satisfied: pathvalidate<4,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.3) (3.2.1)\nRequirement already satisfied: tabledata<2,>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.3) (1.3.3)\nRequirement already satisfied: tcolorpy<1,>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.3) (0.1.6)\nRequirement already satisfied: typepy<2,>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.3) (1.3.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.3) (1.3.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.3) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.3) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.3) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.3) (4.0.3)\nRequirement already satisfied: chardet<6,>=3.0.4 in /opt/conda/lib/python3.10/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.3) (5.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.26.0->lm_eval==0.4.3) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.3) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.3) (2024.7.4)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2018.9 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.3) (2024.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8->lm_eval==0.4.3) (2.1.5)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.3) (2024.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8->lm_eval==0.4.3) (1.3.0)\nBuilding wheels for collected packages: lm_eval\n  Building editable for lm_eval (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lm_eval: filename=lm_eval-0.4.3-0.editable-py3-none-any.whl size=18651 sha256=e85671129c6e7525e13afa7234141504527e604bc2f0ed6ce5cd55cc3e2ef537\n  Stored in directory: /tmp/pip-ephem-wheel-cache-yyuzqxhw/wheels/69/74/5e/8f386e478460b9fbee7a77ea47a97fdb6d9bf8a8a03bae00cf\nSuccessfully built lm_eval\nInstalling collected packages: lm_eval\n  Attempting uninstall: lm_eval\n    Found existing installation: lm_eval 0.4.3\n    Uninstalling lm_eval-0.4.3:\n      Successfully uninstalled lm_eval-0.4.3\nSuccessfully installed lm_eval-0.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nos.environ[\"HF_TOKEN\"]=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Evaluating HuggingFace SmolLM-135M-Instruct\"\nos.environ[\"WANDB_NAME\"] = \"eva-smollm-135M-instruct\"\nos.environ[\"MODEL_NAME\"] = \"smollm-360M-instruct-v0.2-Q8_0-GGUF\"\n\n\nlogin(os.environ[\"HF_TOKEN\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-04T16:23:51.985646Z","iopub.execute_input":"2024-09-04T16:23:51.986230Z","iopub.status.idle":"2024-09-04T16:23:52.811485Z","shell.execute_reply.started":"2024-09-04T16:23:51.986155Z","shell.execute_reply":"2024-09-04T16:23:52.810245Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# !lm_eval --tasks list","metadata":{"execution":{"iopub.status.busy":"2024-09-04T16:23:52.813211Z","iopub.execute_input":"2024-09-04T16:23:52.813640Z","iopub.status.idle":"2024-09-04T16:23:52.818757Z","shell.execute_reply.started":"2024-09-04T16:23:52.813598Z","shell.execute_reply":"2024-09-04T16:23:52.817497Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"%cd\n!pip install cmake\n!git clone https://github.com/ggerganov/llama.cpp.git\n%cd llama.cpp/\n!cmake -B build\n!cmake --build build --config Release\n%cd build/bin/\n!wget https://huggingface.co/HuggingFaceTB/smollm-360M-instruct-v0.2-Q8_0-GGUF/resolve/main/smollm-360m-instruct-add-basics-q8_0.gguf\n!./llama-server -m smollm-360m-instruct-add-basics-q8_0.gguf --port 8080","metadata":{"execution":{"iopub.status.busy":"2024-09-04T16:23:52.820081Z","iopub.execute_input":"2024-09-04T16:23:52.820490Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/root\nCollecting cmake\n  Downloading cmake-3.30.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\nDownloading cmake-3.30.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: cmake\nSuccessfully installed cmake-3.30.2\nCloning into 'llama.cpp'...\nremote: Enumerating objects: 33133, done.\u001b[K\nremote: Counting objects: 100% (8822/8822), done.\u001b[K\nremote: Compressing objects: 100% (728/728), done.\u001b[K\nremote: Total 33133 (delta 8439), reused 8198 (delta 8067), pack-reused 24311 (from 1)\u001b[K\nReceiving objects: 100% (33133/33133), 56.89 MiB | 22.37 MiB/s, done.\nResolving deltas: 100% (23937/23937), done.\n/root/llama.cpp\n-- The C compiler identification is GNU 9.4.0\n-- The CXX compiler identification is GNU 9.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.25.1\")\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n-- Check if compiler accepts -pthread\n-- Check if compiler accepts -pthread - yes\n-- Found Threads: TRUE\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- OpenMP found\n-- Using llamafile\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- x86 detected\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Configuring done (2.4s)\n-- Generating done (0.3s)\n-- Build files have been written to: /root/llama.cpp/build\n[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o\u001b[0m\n[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o\u001b[0m\n[  5%] \u001b[32m\u001b[1mLinking CXX shared library libggml.so\u001b[0m\n[  5%] Built target ggml\n[  5%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n[  9%] \u001b[32m\u001b[1mLinking CXX shared library libllama.so\u001b[0m\n[  9%] Built target llama\n[  9%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n-- Found Git: /usr/bin/git (found version \"2.25.1\")\n[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n[ 10%] Built target build_info\n[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n[ 11%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n[ 11%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n[ 12%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n[ 14%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n[ 14%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n[ 14%] Built target common\n[ 15%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n[ 16%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n[ 16%] Built target test-tokenizer-0\n[ 16%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n[ 17%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n[ 17%] Built target test-tokenizer-1-bpe\n[ 17%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n[ 18%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n[ 18%] Built target test-tokenizer-1-spm\n[ 19%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n[ 19%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n[ 20%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n[ 20%] Built target test-quantize-fns\n[ 21%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n[ 21%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n[ 22%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n[ 22%] Built target test-quantize-perf\n[ 22%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n[ 23%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n[ 23%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n[ 23%] Built target test-sampling\n[ 23%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n[ 25%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n[ 25%] Built target test-chat-template\n[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n[ 27%] Built target test-grammar-parser\n[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n[ 28%] Built target test-llama-grammar\n[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n[ 30%] Built target test-grammar-integration\n[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o\u001b[0m\n[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n[ 31%] Built target test-grad0\n[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n[ 33%] Built target test-backend-ops\n[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n[ 35%] Built target test-rope\n[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n[ 37%] Built target test-model-load-cancel\n[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n[ 39%] Built target test-autorelease\n[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n[ 41%] Built target test-json-schema-to-grammar\n[ 41%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n[ 42%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n[ 42%] Built target test-c\n[ 43%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n[ 43%] Built target llama-cvector-generator\n[ 43%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/llama-baby-llama.dir/baby-llama.cpp.o\u001b[0m\n[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-baby-llama\u001b[0m\n[ 44%] Built target llama-baby-llama\n[ 45%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n[ 45%] Built target llama-batched-bench\n[ 45%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n[ 46%] Built target llama-batched\n[ 47%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/llama-bench-matmult.dir/benchmark-matmult.cpp.o\u001b[0m\n[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench-matmult\u001b[0m\n[ 48%] Built target llama-bench-matmult\n[ 48%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n[ 49%] Built target llama-convert-llama2c-to-ggml\n[ 50%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n[ 50%] Built target llama-embedding\n[ 51%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n[ 52%] Built target llama-eval-callback\n[ 52%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n[ 53%] Built target llama-export-lora\n[ 53%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n[ 54%] Built target llama-gbnf-validator\n[ 54%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n[ 54%] Built target sha256\n[ 55%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n[ 55%] Built target xxhash\n[ 56%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n[ 56%] Built target sha1\n[ 57%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n[ 57%] Built target llama-gguf-hash\n[ 58%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n[ 58%] Built target llama-gguf-split\n[ 58%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n[ 59%] Built target llama-gguf\n[ 60%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n[ 61%] Built target llama-gritlm\n[ 61%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n[ 62%] Built target llama-imatrix\n[ 62%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n[ 63%] Built target llama-infill\n[ 64%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n[ 64%] Built target llama-bench\n[ 64%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n[ 65%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n[ 65%] Built target llava\n[ 66%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n[ 66%] Built target llava_static\n[ 66%] \u001b[32m\u001b[1mLinking CXX shared library libllava_shared.so\u001b[0m\n[ 66%] Built target llava_shared\n[ 67%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n[ 67%] Built target llama-llava-cli\n[ 68%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n[ 68%] Built target llama-minicpmv-cli\n[ 69%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n[ 69%] Built target llama-lookahead\n[ 70%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n[ 71%] Built target llama-lookup\n[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n[ 72%] Built target llama-lookup-create\n[ 72%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n[ 73%] Built target llama-lookup-merge\n[ 74%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n[ 74%] Built target llama-lookup-stats\n[ 74%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n[ 75%] Built target llama-cli\n[ 76%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n[ 76%] Built target llama-parallel\n[ 77%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n[ 78%] Built target llama-passkey\n[ 78%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n[ 79%] Built target llama-perplexity\n[ 80%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n[ 80%] Built target llama-quantize-stats\n[ 81%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n[ 81%] Built target llama-quantize\n[ 82%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n[ 83%] Built target llama-retrieval\n[ 83%] \u001b[34m\u001b[1mGenerating theme-snowstorm.css.hpp\u001b[0m\n[ 83%] \u001b[34m\u001b[1mGenerating colorthemes.css.hpp\u001b[0m\n[ 84%] \u001b[34m\u001b[1mGenerating completion.js.hpp\u001b[0m\n[ 85%] \u001b[34m\u001b[1mGenerating index-new.html.hpp\u001b[0m\n[ 86%] \u001b[34m\u001b[1mGenerating index.html.hpp\u001b[0m\n[ 86%] \u001b[34m\u001b[1mGenerating index.js.hpp\u001b[0m\n[ 87%] \u001b[34m\u001b[1mGenerating json-schema-to-grammar.mjs.hpp\u001b[0m\n[ 88%] \u001b[34m\u001b[1mGenerating prompt-formats.js.hpp\u001b[0m\n[ 89%] \u001b[34m\u001b[1mGenerating style.css.hpp\u001b[0m\n[ 89%] \u001b[34m\u001b[1mGenerating system-prompts.js.hpp\u001b[0m\n[ 90%] \u001b[34m\u001b[1mGenerating theme-beeninorder.css.hpp\u001b[0m\n[ 90%] \u001b[34m\u001b[1mGenerating theme-ketivah.css.hpp\u001b[0m\n[ 91%] \u001b[34m\u001b[1mGenerating theme-mangotango.css.hpp\u001b[0m\n[ 91%] \u001b[34m\u001b[1mGenerating theme-playground.css.hpp\u001b[0m\n[ 92%] \u001b[34m\u001b[1mGenerating theme-polarnight.css.hpp\u001b[0m\n[ 92%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n[ 93%] Built target llama-server\n[ 93%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n[ 94%] Built target llama-save-load-state\n[ 94%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n[ 95%] Built target llama-simple\n[ 96%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n[ 96%] Built target llama-speculative\n[ 97%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n[ 97%] Built target llama-tokenize\n[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n[ 99%] Built target llama-vdot\n[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n[100%] Built target llama-q8dot\n/root/llama.cpp/build/bin\n--2024-09-04 16:30:33--  https://huggingface.co/HuggingFaceTB/smollm-360M-instruct-v0.2-Q8_0-GGUF/resolve/main/smollm-360m-instruct-add-basics-q8_0.gguf\nResolving huggingface.co (huggingface.co)... 13.35.7.81, 13.35.7.5, 13.35.7.38, ...\nConnecting to huggingface.co (huggingface.co)|13.35.7.81|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs-us-1.huggingface.co/repos/38/9a/389a18ecd2c4ed25bb3798488e4e8b258180f884df9ae6e2c7a0e8000ee9081e/b5a2e94a0be8c047bccc3f52bc2f27b79b0dbc688ef3102de54fa6a33b20198c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27smollm-360m-instruct-add-basics-q8_0.gguf%3B+filename%3D%22smollm-360m-instruct-add-basics-q8_0.gguf%22%3B&Expires=1725726633&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNTcyNjYzM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzM4LzlhLzM4OWExOGVjZDJjNGVkMjViYjM3OTg0ODhlNGU4YjI1ODE4MGY4ODRkZjlhZTZlMmM3YTBlODAwMGVlOTA4MWUvYjVhMmU5NGEwYmU4YzA0N2JjY2MzZjUyYmMyZjI3Yjc5YjBkYmM2ODhlZjMxMDJkZTU0ZmE2YTMzYjIwMTk4Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=pXnTQ3cpFJuri5gCwomPTIgtqXpCkGbR-NmH5pRevLNwPx9H6lZ9ZFeL7WnUQZsSGHPA28SjtistbwKuo74SWw8JmLIUoE6HeSnzj2F40W3sfc0aJtj8j78m01ETd1GzFiQLPVoaa3KqYnGZgJqWSQXwpPOpLxdfUQ2jaqt6pJ2xobP1hiEi3YFGMgD30eitkhJ6I9bgNAKw00rxy7dSh01znPPvpa88Js9ranPzLDmii6KHQGbRkncQ%7ELzkWgOKKh4mr50CHCV7cH1M9U0a2EsJwWjDqrowGiTGARRq6TrSrowZgsPkofUs7EHj6RKw2eCVoqSz%7EkflSxI4cNeOKQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n--2024-09-04 16:30:33--  https://cdn-lfs-us-1.huggingface.co/repos/38/9a/389a18ecd2c4ed25bb3798488e4e8b258180f884df9ae6e2c7a0e8000ee9081e/b5a2e94a0be8c047bccc3f52bc2f27b79b0dbc688ef3102de54fa6a33b20198c?response-content-disposition=inline%3B+filename*%3DUTF-8''smollm-360m-instruct-add-basics-q8_0.gguf%3B+filename%3D%22smollm-360m-instruct-add-basics-q8_0.gguf%22%3B&Expires=1725726633&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNTcyNjYzM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzM4LzlhLzM4OWExOGVjZDJjNGVkMjViYjM3OTg0ODhlNGU4YjI1ODE4MGY4ODRkZjlhZTZlMmM3YTBlODAwMGVlOTA4MWUvYjVhMmU5NGEwYmU4YzA0N2JjY2MzZjUyYmMyZjI3Yjc5YjBkYmM2ODhlZjMxMDJkZTU0ZmE2YTMzYjIwMTk4Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=pXnTQ3cpFJuri5gCwomPTIgtqXpCkGbR-NmH5pRevLNwPx9H6lZ9ZFeL7WnUQZsSGHPA28SjtistbwKuo74SWw8JmLIUoE6HeSnzj2F40W3sfc0aJtj8j78m01ETd1GzFiQLPVoaa3KqYnGZgJqWSQXwpPOpLxdfUQ2jaqt6pJ2xobP1hiEi3YFGMgD30eitkhJ6I9bgNAKw00rxy7dSh01znPPvpa88Js9ranPzLDmii6KHQGbRkncQ~LzkWgOKKh4mr50CHCV7cH1M9U0a2EsJwWjDqrowGiTGARRq6TrSrowZgsPkofUs7EHj6RKw2eCVoqSz~kflSxI4cNeOKQ__&Key-Pair-Id=K24J24Z295AEI9\nResolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 13.35.35.127, 13.35.35.21, 13.35.35.125, ...\nConnecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|13.35.35.127|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 386405440 (369M) [binary/octet-stream]\nSaving to: 'smollm-360m-instruct-add-basics-q8_0.gguf'\n\n0.gguf               66%[============>       ] 243.24M  25.8MB/s    eta 6s     ","output_type":"stream"}]},{"cell_type":"code","source":"!lm_eval --model hf \\\n    --model_args model=${MODEL_NAME},base_url=http://127.0.0.1:8000/v1/chat/completions  \\\n    --tasks mmlu \\\n    --device cuda:0 \\\n    --num_fewshot 0 \\\n    --batch_size 4 \\\n    --model gguf \\\n    --output_path results \\\n    --use_cache True\\\n    --log_samples \\\n    --limit 10 \n    #--hf_hub_log_args hub_results_org=micost,hub_repo_name=eval-smolLM-135M,push_results_to_hub=True,push_samples_to_hub=True,public_repo=False","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}