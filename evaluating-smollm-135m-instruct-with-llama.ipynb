{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":198397091,"sourceType":"kernelVersion"},{"sourceId":120932,"sourceType":"modelInstanceVersion","modelInstanceId":101729,"modelId":125947},{"sourceId":120933,"sourceType":"modelInstanceVersion","modelInstanceId":101730,"modelId":125947}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\nThis note evaluate smolLM-135M instruct model with llama.cpp\n\n# Env and tools prepare","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/llama-cpp-binary-compiled-with-gpu/llama.cpp /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:41:23.923524Z","iopub.execute_input":"2024-09-28T14:41:23.923849Z","iopub.status.idle":"2024-09-28T14:42:48.080650Z","shell.execute_reply.started":"2024-09-28T14:41:23.923816Z","shell.execute_reply":"2024-09-28T14:42:48.079360Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!chmod +x -R llama.cpp\n%cd llama.cpp\n!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:42:48.082805Z","iopub.execute_input":"2024-09-28T14:42:48.083155Z","iopub.status.idle":"2024-09-28T14:43:08.208587Z","shell.execute_reply.started":"2024-09-28T14:42:48.083119Z","shell.execute_reply":"2024-09-28T14:43:08.207161Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/llama.cpp\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Run perplexity\n\nPerplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens.\nFormally, the perplexity of LLMs is the exponentiated average negative log-likelihood,  which means **perplexity smaller is better** \n\nPerplexity benchmark shows how accurate the model is ","metadata":{}},{"cell_type":"code","source":"\n!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_V2.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > pip_fp16.log 2>&1\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T14:43:08.210603Z","iopub.execute_input":"2024-09-28T14:43:08.211719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tail -6 ppl_Q4_V2.log\n!tail -6 ppl_Q4.log\n!tail -6 pip_fp16.log","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Benchmarking the Inference Throughput and Memory Consumption \n\nLlama-bench shows performance and resource consumption. \nThere are three types of test\n- Prompt processing (pp): processing a prompt in batches (-p)\n- Text generation (tg): generating a sequence of tokens (-n)\n- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\n\nIn the following test, we run text generation and prompt processing\n","metadata":{}},{"cell_type":"code","source":"!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 ","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        108.06 ± 3.09 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12402.11 ± 518.29 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        164.74 ± 0.57 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        163.21 ± 1.35 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        157.77 ± 2.31 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13839.79 ± 28.18 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        223.03 ± 0.40 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        220.56 ± 0.23 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        213.05 ± 1.08 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      7893.69 ± 29.10 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         80.82 ± 6.73 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         86.18 ± 0.53 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         82.22 ± 1.47 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     9579.20 ± 249.97 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        111.86 ± 1.91 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        106.05 ± 4.93 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        104.86 ± 1.18 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12145.43 ± 294.12 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |       144.60 ± 15.37 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        157.46 ± 1.91 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        155.95 ± 1.19 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13704.81 ± 13.29 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        221.85 ± 3.60 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        219.16 ± 2.74 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        214.20 ± 0.70 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      5866.77 ± 46.54 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         77.86 ± 0.55 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         73.92 ± 2.66 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         72.32 ± 1.56 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |      7914.09 ± 96.49 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        101.47 ± 0.42 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |         95.78 ± 6.20 |\n","output_type":"stream"}]}]}