{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa78da82",
   "metadata": {
    "papermill": {
     "duration": 0.003252,
     "end_time": "2024-09-28T14:52:57.758650",
     "exception": false,
     "start_time": "2024-09-28T14:52:57.755398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction \n",
    "\n",
    "This note evaluate smolLM-135M instruct model with llama.cpp\n",
    "\n",
    "# Env and tools prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134c6942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T14:52:57.765869Z",
     "iopub.status.busy": "2024-09-28T14:52:57.765482Z",
     "iopub.status.idle": "2024-09-28T14:53:56.296512Z",
     "shell.execute_reply": "2024-09-28T14:53:56.295220Z"
    },
    "papermill": {
     "duration": 58.538789,
     "end_time": "2024-09-28T14:53:56.300397",
     "exception": false,
     "start_time": "2024-09-28T14:52:57.761608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/llama-cpp-binary-compiled-with-gpu/llama.cpp /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f0548b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T14:53:56.313705Z",
     "iopub.status.busy": "2024-09-28T14:53:56.313295Z",
     "iopub.status.idle": "2024-09-28T14:54:16.260990Z",
     "shell.execute_reply": "2024-09-28T14:54:16.259984Z"
    },
    "papermill": {
     "duration": 19.956752,
     "end_time": "2024-09-28T14:54:16.263466",
     "exception": false,
     "start_time": "2024-09-28T14:53:56.306714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "!chmod +x -R llama.cpp\n",
    "%cd llama.cpp\n",
    "!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b3e4d",
   "metadata": {
    "papermill": {
     "duration": 0.002589,
     "end_time": "2024-09-28T14:54:16.269335",
     "exception": false,
     "start_time": "2024-09-28T14:54:16.266746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run perplexity\n",
    "\n",
    "Perplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens.\n",
    "Formally, the perplexity of LLMs is the exponentiated average negative log-likelihood,  which means **perplexity smaller is better** \n",
    "\n",
    "Perplexity benchmark shows how accurate the model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112e9ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T14:54:16.276807Z",
     "iopub.status.busy": "2024-09-28T14:54:16.276119Z",
     "iopub.status.idle": "2024-09-28T14:56:59.516092Z",
     "shell.execute_reply": "2024-09-28T14:56:59.514939Z"
    },
    "papermill": {
     "duration": 163.246382,
     "end_time": "2024-09-28T14:56:59.518617",
     "exception": false,
     "start_time": "2024-09-28T14:54:16.272235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_V2.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > pip_fp16.log 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9851f538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T14:56:59.526157Z",
     "iopub.status.busy": "2024-09-28T14:56:59.525794Z",
     "iopub.status.idle": "2024-09-28T14:57:02.447818Z",
     "shell.execute_reply": "2024-09-28T14:57:02.446824Z"
    },
    "papermill": {
     "duration": 2.928561,
     "end_time": "2024-09-28T14:57:02.450336",
     "exception": false,
     "start_time": "2024-09-28T14:56:59.521775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final estimate: PPL = 23.2651 +/- 0.28000\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =    1725.50 ms\r\n",
      "llama_perf_context_print: prompt eval time =   28434.42 ms / 131072 tokens (    0.22 ms per token,  4609.62 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =   48388.05 ms / 131073 tokens\r\n",
      "Final estimate: PPL = 23.2651 +/- 0.28000\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =    1614.24 ms\r\n",
      "llama_perf_context_print: prompt eval time =   28152.20 ms / 131072 tokens (    0.21 ms per token,  4655.83 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =   47522.84 ms / 131073 tokens\r\n",
      "Final estimate: PPL = 21.2879 +/- 0.25264\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =    3105.68 ms\r\n",
      "llama_perf_context_print: prompt eval time =   38220.95 ms / 131072 tokens (    0.29 ms per token,  3429.32 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =   57345.06 ms / 131073 tokens\r\n"
     ]
    }
   ],
   "source": [
    "!tail -6 ppl_Q4_V2.log\n",
    "!tail -6 ppl_Q4.log\n",
    "!tail -6 pip_fp16.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de21f27",
   "metadata": {
    "papermill": {
     "duration": 0.003047,
     "end_time": "2024-09-28T14:57:02.456864",
     "exception": false,
     "start_time": "2024-09-28T14:57:02.453817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Benchmarking the Inference Throughput and Memory Consumption \n",
    "\n",
    "Llama-bench shows performance and resource consumption. \n",
    "There are three types of test\n",
    "- Prompt processing (pp): processing a prompt in batches (-p)\n",
    "- Text generation (tg): generating a sequence of tokens (-n)\n",
    "- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\n",
    "\n",
    "In the following test, we run text generation and prompt processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4187c7a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T14:57:02.465625Z",
     "iopub.status.busy": "2024-09-28T14:57:02.464754Z",
     "iopub.status.idle": "2024-09-28T15:04:32.386225Z",
     "shell.execute_reply": "2024-09-28T15:04:32.385060Z"
    },
    "papermill": {
     "duration": 449.92841,
     "end_time": "2024-09-28T15:04:32.388556",
     "exception": false,
     "start_time": "2024-09-28T14:57:02.460146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\n",
      "ggml_cuda_init: found 1 CUDA devices:\r\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\r\n",
      "| model                          |       size |     params | backend    | ngl |   main_gpu |          test |                  t/s |\r\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------: | -------------------: |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |     7785.73 ± 183.27 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         88.32 ± 4.24 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         89.16 ± 1.19 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         84.95 ± 1.64 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     9456.34 ± 265.26 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        115.17 ± 1.96 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        112.94 ± 1.79 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        107.42 ± 3.81 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12493.86 ± 442.58 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        163.45 ± 2.41 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        155.95 ± 4.85 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        157.00 ± 3.30 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13825.22 ± 30.56 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        224.77 ± 0.25 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        221.03 ± 1.07 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        215.13 ± 1.07 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      8004.29 ± 29.76 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         91.58 ± 1.30 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         88.82 ± 0.70 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         83.86 ± 2.34 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     9697.52 ± 258.08 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        109.22 ± 7.53 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        110.03 ± 0.72 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        105.76 ± 3.12 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12231.95 ± 288.84 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        158.82 ± 2.48 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        163.47 ± 2.86 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        158.04 ± 2.32 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13660.18 ± 91.22 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        222.59 ± 2.72 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        221.21 ± 1.40 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        215.31 ± 0.47 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      5934.95 ± 40.46 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         76.32 ± 3.80 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         75.69 ± 1.66 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         71.96 ± 1.20 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     7929.88 ± 206.95 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |         99.64 ± 2.98 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |         96.71 ± 2.45 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |         94.23 ± 1.21 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12210.62 ± 436.55 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        146.58 ± 3.68 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        149.84 ± 2.64 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        146.06 ± 2.59 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13733.66 ± 43.77 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        238.99 ± 2.46 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        238.10 ± 0.21 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        230.32 ± 0.43 |\r\n",
      "\r\n",
      "build: 95bc82fb (3828)\r\n"
     ]
    }
   ],
   "source": [
    "!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 198397091,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 125947,
     "modelInstanceId": 101729,
     "sourceId": 120932,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 125947,
     "modelInstanceId": 101730,
     "sourceId": 120933,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 697.654322,
   "end_time": "2024-09-28T15:04:32.714382",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-28T14:52:55.060060",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
