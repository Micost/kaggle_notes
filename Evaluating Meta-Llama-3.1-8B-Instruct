{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"This is a sample note about how to evaluate language model which is directly copied from Aisuko's note \"Evaluating language models \"","metadata":{}},{"cell_type":"code","source":"# !pip install -U -q lm-eval==0.4.3","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:28:33.161870Z","iopub.execute_input":"2024-09-08T07:28:33.162805Z","iopub.status.idle":"2024-09-08T07:28:33.167104Z","shell.execute_reply.started":"2024-09-08T07:28:33.162751Z","shell.execute_reply":"2024-09-08T07:28:33.166087Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/EleutherAI/lm-evaluation-harness\n%cd lm-evaluation-harness\n!pip install --quiet -e .","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:28:33.170902Z","iopub.execute_input":"2024-09-08T07:28:33.171190Z","iopub.status.idle":"2024-09-08T07:29:09.336455Z","shell.execute_reply.started":"2024-09-08T07:28:33.171160Z","shell.execute_reply":"2024-09-08T07:29:09.335325Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Cloning into 'lm-evaluation-harness'...\nremote: Enumerating objects: 40293, done.\u001b[K\nremote: Counting objects: 100% (622/622), done.\u001b[K\nremote: Compressing objects: 100% (398/398), done.\u001b[K\nremote: Total 40293 (delta 324), reused 486 (delta 223), pack-reused 39671 (from 1)\u001b[K\nReceiving objects: 100% (40293/40293), 27.00 MiB | 19.02 MiB/s, done.\nResolving deltas: 100% (28238/28238), done.\n/kaggle/working/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nos.environ[\"HF_TOKEN\"]=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Evaluating allenai Meta-Llama-3.1-8B-Instruct\"\nos.environ[\"WANDB_NAME\"] = \"Meta-Llama-3.1-8B-Instruct\"\nos.environ[\"MODEL_NAME\"] = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\nos.environ[\"DATASET\"] = \"HuggingFaceH4/ultrafeedback_binarized\"\n\nlogin(os.environ[\"HF_TOKEN\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:29:09.339136Z","iopub.execute_input":"2024-09-08T07:29:09.339571Z","iopub.status.idle":"2024-09-08T07:29:11.335040Z","shell.execute_reply.started":"2024-09-08T07:29:09.339522Z","shell.execute_reply":"2024-09-08T07:29:11.334129Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"#!lm_eval --help","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-08T07:37:21.369138Z","iopub.execute_input":"2024-09-08T07:37:21.370066Z","iopub.status.idle":"2024-09-08T07:37:31.560907Z","shell.execute_reply.started":"2024-09-08T07:37:21.370020Z","shell.execute_reply":"2024-09-08T07:37:31.559744Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"usage: lm_eval [-h] [--model MODEL] [--tasks task1,task2]\n               [--model_args MODEL_ARGS] [--num_fewshot N]\n               [--batch_size auto|auto:N|N] [--max_batch_size N]\n               [--device DEVICE] [--output_path DIR|DIR/file.json]\n               [--limit N|0<N<1] [--use_cache DIR]\n               [--cache_requests {true,refresh,delete}] [--check_integrity]\n               [--write_out] [--log_samples]\n               [--system_instruction SYSTEM_INSTRUCTION]\n               [--apply_chat_template [APPLY_CHAT_TEMPLATE]]\n               [--fewshot_as_multiturn] [--show_config] [--include_path DIR]\n               [--gen_kwargs GEN_KWARGS]\n               [--verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG]\n               [--wandb_args WANDB_ARGS] [--hf_hub_log_args HF_HUB_LOG_ARGS]\n               [--predict_only] [--seed SEED] [--trust_remote_code]\n\noptions:\n  -h, --help            show this help message and exit\n  --model MODEL, -m MODEL\n                        Name of model e.g. `hf`\n  --tasks task1,task2, -t task1,task2\n                        Comma-separated list of task names or task groupings to evaluate on.\n                        To get full list of tasks, use one of the commands `lm-eval --tasks {{list_groups,list_subtasks,list_tags,list}}` to list out all available names for task groupings; only (sub)tasks; tags; or all of the above\n  --model_args MODEL_ARGS, -a MODEL_ARGS\n                        Comma separated string arguments for model, e.g. `pretrained=EleutherAI/pythia-160m,dtype=float32`\n  --num_fewshot N, -f N\n                        Number of examples in few-shot context\n  --batch_size auto|auto:N|N, -b auto|auto:N|N\n                        Acceptable values are 'auto', 'auto:N' or N, where N is an integer. Default 1.\n  --max_batch_size N    Maximal batch size to try with --batch_size auto.\n  --device DEVICE       Device to use (e.g. cuda, cuda:0, cpu).\n  --output_path DIR|DIR/file.json, -o DIR|DIR/file.json\n                        The path to the output file where the result metrics will be saved. If the path is a directory and log_samples is true, the results will be saved in the directory. Else the parent directory will be used.\n  --limit N|0<N<1, -L N|0<N<1\n                        Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.\n  --use_cache DIR, -c DIR\n                        A path to a sqlite db file for caching model responses. `None` if not caching.\n  --cache_requests {true,refresh,delete}\n                        Speed up evaluation by caching the building of dataset requests. `None` if not caching.\n  --check_integrity     Whether to run the relevant part of the test suite for the tasks.\n  --write_out, -w       Prints the prompt for the first few documents.\n  --log_samples, -s     If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis. Use with --output_path.\n  --system_instruction SYSTEM_INSTRUCTION\n                        System instruction to be used in the prompt\n  --apply_chat_template [APPLY_CHAT_TEMPLATE]\n                        If True, apply chat template to the prompt. Providing `--apply_chat_template` without an argument will apply the default chat template to the prompt. To apply a specific template from the available list of templates, provide the template name as an argument. E.g. `--apply_chat_template template_name`\n  --fewshot_as_multiturn\n                        If True, uses the fewshot as a multi-turn conversation\n  --show_config         If True, shows the the full config of all tasks at the end of the evaluation.\n  --include_path DIR    Additional path to include if there are external tasks to include.\n  --gen_kwargs GEN_KWARGS\n                        String arguments for model generation on greedy_until tasks, e.g. `temperature=0,top_k=0,top_p=0`.\n  --verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG, -v CRITICAL|ERROR|WARNING|INFO|DEBUG\n                        Controls the reported logging error level. Set to DEBUG when testing + adding new task configurations for comprehensive log output.\n  --wandb_args WANDB_ARGS\n                        Comma separated string arguments passed to wandb.init, e.g. `project=lm-eval,job_type=eval\n  --hf_hub_log_args HF_HUB_LOG_ARGS\n                        Comma separated string arguments passed to Hugging Face Hub's log function, e.g. `hub_results_org=EleutherAI,hub_repo_name=lm-eval-results`\n  --predict_only, -x    Use with --log_samples. Only model outputs will be saved and metrics will not be evaluated.\n  --seed SEED           Set seed for python's random, numpy, torch, and fewshot sampling.\n                        Accepts a comma-separated list of 4 values for python's random, numpy, torch, and fewshot sampling seeds, respectively, or a single integer to set the same seed for all four.\n                        The values are either an integer or 'None' to not set the seed. Default is `0,1234,1234,1234` (for backward compatibility).\n                        E.g. `--seed 0,None,8,52` sets `random.seed(0)`, `torch.manual_seed(8)`, and fewshot sampling seed to 52. Here numpy's seed is not set since the second value is `None`.\n                        E.g, `--seed 42` sets all four seeds to 42.\n  --trust_remote_code   Sets trust_remote_code to True to execute code to create HF Datasets from the Hub\n","output_type":"stream"}]},{"cell_type":"code","source":"!lm_eval --model hf \\\n    --model_args pretrained=${MODEL_NAME} \\\n    --tasks mmlu \\\n    --device cuda:0 \\\n    --num_fewshot 0 \\\n    --batch_size 4 \\\n    --output_path results \\\n    --use_cache True\\\n    --log_samples \\\n    --limit 10 \\\n#     --hf_hub_log_args hub_results_org=aisuko,hub_repo_name=eval-smolLM-135M,push_results_to_hub=True,push_samples_to_hub=True,public_repo=False","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:29:11.343832Z","iopub.execute_input":"2024-09-08T07:29:11.344158Z","iopub.status.idle":"2024-09-08T07:29:37.923076Z","shell.execute_reply.started":"2024-09-08T07:29:11.344127Z","shell.execute_reply":"2024-09-08T07:29:37.922134Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/opt/conda/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1347, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n    raise head_call_error\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1751, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1673, in get_hf_file_metadata\n    r = _request_wrapper(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 376, in _request_wrapper\n    response = _request_wrapper(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 400, in _request_wrapper\n    hf_raise_for_status(response)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\n    raise GatedRepoError(message, response) from e\nhuggingface_hub.utils._errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66dd5260-1797eb03107d24705c02f7d2;6b16d2e4-81cf-42fb-9e91-199d56951dc6)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct to ask for access.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm_eval/__main__.py\", line 382, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm_eval/utils.py\", line 397, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm_eval/evaluator.py\", line 201, in simple_evaluate\n    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n  File \"/kaggle/working/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm_eval/api/model.py\", line 147, in create_from_arg_string\n    return cls(**args, **args2)\n  File \"/kaggle/working/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 161, in __init__\n    self._get_config(\n  File \"/kaggle/working/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 499, in _get_config\n    self._config = transformers.AutoConfig.from_pretrained(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 976, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 420, in cached_file\n    raise EnvironmentError(\nOSError: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n403 Client Error. (Request ID: Root=1-66dd5260-1797eb03107d24705c02f7d2;6b16d2e4-81cf-42fb-9e91-199d56951dc6)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct to ask for access.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Related issues\n\n* https://github.com/EleutherAI/lm-evaluation-harness/issues/2263","metadata":{}}]}